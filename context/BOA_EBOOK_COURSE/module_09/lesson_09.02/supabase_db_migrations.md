---
title: Supabase db migrations
description: Let's use the Supabase CLI to manage our database migrations and schemas.
code: https://gitlab.com/fullstackio/books/newline-course-apps/giorgio-boa-qwik-in-action-app/-/tree/main/module_09
---

# Supabase db Migrations

Firstly, you'll want to delete the `test` table that was created, and then create a product table. This can be done in a precise and punctual manner, thanks to the automatic Supabase tool, to avoid carrying out operations manually.
To create a new migration, initiate everything from the terminal using the command:

```shell
pnpm supabase migration new products
```

A file will be created in the migration folder:

```shell
qwik-e-commerce
├── supabase
│   ├── migrations
│   │   └── <timestamp>_products.sql
```

There are other files managed independently by Supabase, but specifically, this file is the focus. In the first part of the `<timestamp>`\_products.sql file, the date of its creation is noted, while in the second <timestamp\>\_`products`.sql, the name that was previously assigned with the command in the terminal is present.
In this file, SQL can be defined, which will be executed to modify the database.
Let's define the table:

```SQL
drop table public.test;

create table public.products (
  id bigint generated by default as identity,
  name text null,
  description text null,
  price double precision null,
  image text null,
  constraint products_pkey primary key (id)
) tablespace pg_default;
```

Here, the fields of the table are being defined, and it can be seen that the id field is incremental, so there's no need to value it; it will be done automatically. Also, the other fields in the table can be noticed.
The information regarding the product needed is:

- product name
- description
- price
- a beautiful image that encourages the customer to buy

So, without wasting time, populate the database with this information.

Populate the `seed.sql` file, which is inside the `supabase` folder.
This file is executed after all migrations. A good practice is not to add schema statements to your seed file, only data.

```SQL
insert into public.products
  (name, description, price, image)
values
  ('Spiky Cactus', 'A spiky yet elegant house cactus - perfect for the home or office. Origin and habitat: Probably native only to the Andes of Peru', 20.95,  'spiky_cactus.webp'),
  ('Tulip Pot', 'Bright crimson red species tulip with black centers, the poppy-like flowers will open up in full sun. Ideal for rock gardens, pots and border edging.', 10.95, 'tulip_pot.webp'),
  ('Aloe Vera', 'Decorative Aloe vera makes a lovely house plant. A really trendy plant, Aloe vera is just so easy to care for.', 10.45, 'aloe_vera.webp'),
  ('Fern Blechnum Gibbum', 'Create a tropical feel in your home with this lush green tree fern, it has decorative leaves and will develop a short slender trunk in time.', 12.95, 'fern_blechnum_gibbum.webp'),
  ('Assorted Indoor Succulents', 'These assorted succulents come in a variety of different shapes and colours - each with their own unique personality.', 42.35, 'assorted_indoor_succulents.webp'),
  ('Orchid', 'Gloriously elegant. It can go along with any interior as it is a neutral color and the most popular Phalaenopsis overall.', 30.75, 'orchid.webp');
```

Here, the database products have been defined. Now, all that's left to do is launch this command:

```shell
pnpm supabase db reset
```

This command recreates the local database from scratch. It runs all the migration scripts present in the `supabase/migrations` directory to recreate the optimal and clean situation. Furthermore, as seen before, it will also insert the data by reading and executing the `seed.sql` file. This automatic process has the advantage of being replicable, and therefore, all developers on the team may be able to set up their environment without problems so they can be productive immediately.

With the `pnpm supabase start` command, a local copy of the environment present in the Supabase cloud has been recreated. There's also the possibility of accessing a dashboard completely identical to the one available in Supabase, but with the advantage that it is local. Therefore, generating the migrations starting from the local dashboard and then generating the migration via the Supabase CLI is also possible.
With this `diff` command, a new migration script from changes already applied to the local database can be created.

```shell
pnpm supabase db diff -f new_migration_name
```

This makes the process even smoother and faster. These migration and seeding files will be part of the project and the versioned code in all respects.

## Align Remote Database

By executing the following command, the database situation can be checked to understand if there are misalignments.

```shell
pnpm supabase migration list

# Output
#
# Connecting to remote database...
#
#       LOCAL      │     REMOTE     │     TIME (UTC)
# ─────────────────┼────────────────┼──────────────────────
#   migration-123  │  migration-123 │   last_update_time
#   migration-456  │                │   last_update_time
```

Here, the situation of the databases can be seen, and it can be noticed that the remote one is not aligned with what is locally present. This is because the `test` table was removed and the `products` one was created, and these changes are not available in the remote DB.
To align the remote database on the Supabase cloud, use this command:

```shell
pnpm supabase db push
```

To carry out an alignment and perform the migrations, at the end of the operation, the `products` table will also be found in the remote database.
But in fact, with this operation, the functionality has not been tied to database modification. It has been seen previously that this is a good practice, and this level of process and alignment between functionality and database can be obtained thanks to the use of GitHub Actions.

## GitHub Action

GitHub offers a feature called GitHub Actions, which gives the ability to launch tasks of any type when a GitHub event occurs. For example, when new code is pushed to the repository according to a pre-established schedule or when an external event occurs using the repository submission webhook.
Often, these tasks are set up in repositories to verify PRs and perform a series of checks before maintainers can even evaluate changes made to the project.

It is possible to use these flows to deploy packages or release projects, too. (e.g., every push on the main branch, the public site relating to the project documentation can be updated) This GitHub functionality will be used to perform migrations of the various environments available in Supabase.
By creating configuration files inside the `.github/workflows` folder, automated procedures can be created, and a real [Continuous Integration](https://en.wikipedia.org/wiki/Continuous_integration) can be established.

So, let's analyze this file:

FILE: `.github/workflows/production.yaml`

```yaml
name: Deploy Migrations to Production

on:
  push:
    branches:
      - main
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest

    env:
      SUPABASE_ACCESS_TOKEN: ${{ secrets.SUPABASE_ACCESS_TOKEN }}
      SUPABASE_DB_PASSWORD: ${{ secrets.PRODUCTION_DB_PASSWORD }}
      SUPABASE_PROJECT_ID: ${{ secrets.PRODUCTION_PROJECT_ID }}

    steps:
      - uses: actions/checkout@v3

      - uses: supabase/setup-cli@v1
        with:
          version: latest

      - run: supabase link --project-ref $SUPABASE_PROJECT_ID
      - run: supabase db push
```

By creating the file inside the previously mentioned folder, the workflow can be defined. Here, it is being said that every time a push is made on the `main` branch, a series of steps with the files that are present inside the project are also performed. The files are checked out, and the project is practically downloaded. With the Supabase CLI, the link operations to the project present in the Supabase cloud and the database push are performed. These are the same operations that were done step by step in the previous section, and the workflow behaves in the same way. This allows the database migration to be performed only at the correct moment in time, only when the application code needs it.

If this process needs to be brought to other environments, too, all that's needed is to create a specific file for the environment and modify the variables and the branch that are included in the configuration.

> **Secrets variables**: It is good practice to save secrets within your GitHub and then use them in configuration files. This allows us to avoid revealing these variables that should not be made public. There is a special section within GitHub where you can create your variables, which can be made visible in different ways within the organization; you can share a variable between multiple repositories or limit its visibility to a single project.
